from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, col, when, year as spark_year, to_date, size
from pyspark.sql.types import IntegerType, DoubleType

print("=" * 60)
print("ğŸ”„ ETL: MONGODB (Streaming Data) â†’ HDFS")
print("=" * 60)

# Khá»Ÿi táº¡o Spark vá»›i MongoDB
spark = SparkSession.builder \
    .appName("MovieETL_Unified") \
    .config("spark.mongodb.read.connection.uri", "mongodb://mongodb:27017/movies.movie_data") \
    .config("spark.mongodb.wri on.uri", "mongodb://mongodb:27017/movies.movies_silver") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Sessin Ä‘Ã£ Ä‘Æ°á»£c táº¡o!")

# Äá»c tá»« MongoDB collection chá»©a data streaming
print("ğŸ“– Äá»c dá»¯ liá»‡u tá»« MongoDB (movies.movie_data)...")
df = spark.read \
    .format("mongodb") \
    .load()

total_records = df.count()
print(f"ğŸ“Š Tá»•ng sá»‘ records tá»« MongoDB: {total_records}")

# Hiá»ƒn thá»‹ schema
print("\nğŸ“‹ Schema tá»« MongoDB:")
df.printSchema()

# Chá»n cÃ¡c cá»™t cáº§n thiáº¿t vÃ  lÃ m sáº¡ch
print("\nğŸ”§ Äang ETL vÃ  lÃ m sáº¡ch dá»¯ liá»‡u...")

#Ã‰p Kiá»ƒu dá»¯ liá»‡u
df_cleaned = df.select(
    col("id").cast(IntegerType()).alias("movie_id"), #Ä‘á»•i tÃªn cá»™t id thÃ nh cá»™t movie_id
    col("title"),
    col("original_title"),
    col("year").cast(IntegerType()),#Ã©p kiá»ƒu sá»‘ nguyÃªn
    col("rating").cast(DoubleType()),#Ã©p kiá»ƒu sá»‘ thá»±c
    col("votes").cast(IntegerType()),#Ã©p kiá»ƒu sá»‘ nguyÃªn
    col("genres"), 
    col("overview"),
    col("popularity").cast(DoubleType()), #Ã©p kiá»ƒu sá»‘ thá»±c
    col("original_language"),
    col("release_date"),
    col("source"),
    col("fetched_at")
)

# Validate vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
df_validated = df_cleaned \
    .withColumn("year", 
                when(col("year").isNull(), 0)
                .when(col("year") < 1900, 0)
                .when(col("year") > 2030, 0)
                .otherwise(col("year"))) \
    .withColumn("rating", 
                when(col("rating").isNull(), 0.0)
                .when(col("rating") < 0, 0.0)
                .when(col("rating") > 10, 0.0)
                .otherwise(col("rating"))) \
    .withColumn("votes", 
                when(col("votes").isNull(), 0)
                .when(col("votes") < 0, 0)
                .otherwise(col("votes"))) \
    .filter(size(col("genres")) > 0)  # Chá»‰ giá»¯ phim cÃ³ Ã­t nháº¥t 1 thá»ƒ loáº¡i

print(f"âœ… Sau khi validate: {df_validated.count()} records")

# Má»—i thá»ƒ loáº¡i táº¡o 1 record riÃªng
print("\nğŸ’¥ TÃ¡ch 1 phim â†’ nhiá»u records")
df_exploded = df_validated \
    .withColumn("genre", explode(col("genres"))) \
    .select(
        "movie_id",
        "title",
        "original_title",
        "year",
        "rating",
        "votes",
        "genre",  
        "overview",
        "popularity",
        "original_language",
        "release_date",
        "source",
        "fetched_at"
    )

# kiá»ƒm tra dá»¯ liá»‡u sau transform
print("\nğŸ“ Máº«u data sau transform:")
df_exploded.select("title", "year", "rating", "votes", "genre").show(20, truncate=False)

# Thá»‘ng kÃª trÆ°á»›c khi ghi
print("\n" + "=" * 60)
print("ğŸ“Š THá»NG KÃŠ DATA")
print("=" * 60)


print(f"Sá»‘ phim unique: {df_exploded.select('movie_id').distinct().count():,}")
print(f"Sá»‘ genres unique: {df_exploded.select('genre').distinct().count()}")

# # Thá»‘ng kÃª theo nÄƒm
# year_stats = df_exploded.groupBy("year").count().orderBy("year", ascending=False)
# print("\nğŸ“… PhÃ¢n bá»‘ theo nÄƒm (Top 10):")
# year_stats.show(10)

# Thá»‘ng kÃª theo genre
# genre_stats = df_exploded.groupBy("genre").count().orderBy("count", ascending=False)
# print("\nğŸ­ PhÃ¢n bá»‘ theo genre:")
# genre_stats.show(20)

# Rating statistics
# rating_stats = df_exploded.select(
#     "rating"
# ).describe()
# print("\nâ­ Thá»‘ng kÃª rating:")
# rating_stats.show()

# LÆ°u vÃ o HDFS dáº¡ng JSONL
hdfs_path_jsonl = "hdfs://namenode:9000/data/movies/silver/movies.jsonl"
print(f"\nğŸ’¾ Äang ghi vÃ o HDFS: {hdfs_path_jsonl}")

df_exploded.write \
    .mode("overwrite") \
    .json(hdfs_path_jsonl) 
print("âœ… ÄÃ£ ghi vÃ o HDFS thÃ nh cÃ´ng!")
print("\n" + "=" * 60)
print("âœ… ETL HOÃ€N Táº¤T!")
print("=" * 60)
print(f"\nğŸ“ Output:")
print(f"   - HDFS Main Data: {hdfs_path_jsonl}")
print(f"\nğŸ¯ Sáºµn sÃ ng cho MapReduce jobs!")
spark.stop()