# ğŸ¬ Movie Data Pipeline - Real-time Streaming & Big Data Analytics

Há»‡ thá»‘ng xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u phim theo thá»i gian thá»±c, sá»­ dá»¥ng kiáº¿n trÃºc Lambda vá»›i Kafka, Spark Streaming, Hadoop MapReduce vÃ  MongoDB.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#-tá»•ng-quan)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [CÃ´ng nghá»‡ sá»­ dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t vÃ  cháº¡y](#-cÃ i-Ä‘áº·t-vÃ -cháº¡y)
- [CÃ¡c thÃ nh pháº§n chÃ­nh](#-cÃ¡c-thÃ nh-pháº§n-chÃ­nh)
- [MapReduce Jobs](#-mapreduce-jobs)
- [API Keys](#-api-keys)
- [Ports vÃ  Services](#-ports-vÃ -services)


## ğŸ¯ Tá»•ng quan

Dá»± Ã¡n xÃ¢y dá»±ng má»™t data pipeline hoÃ n chá»‰nh Ä‘á»ƒ:
- **Thu tháº­p** dá»¯ liá»‡u phim tá»« TMDB API theo thá»i gian thá»±c
- **Streaming** dá»¯ liá»‡u qua Apache Kafka
- **Xá»­ lÃ½ real-time** vá»›i Spark Structured Streaming
- **LÆ°u trá»¯** trong MongoDB (NoSQL)
- **ETL** tá»« MongoDB sang HDFS
- **PhÃ¢n tÃ­ch batch** vá»›i Hadoop MapReduce

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
TMDB API
    â†“
[Producer] â†’ Kafka â†’ [Spark Streaming] â†’ MongoDB
                                              â†“
                                          [ETL Job]
                                              â†“
                                            HDFS
                                              â†“
                                      [MapReduce Jobs]
                                              â†“
                                         Analytics
```

### Luá»“ng dá»¯ liá»‡u chi tiáº¿t:

1. **Data Ingestion (Real-time)**
   - Producer gá»i TMDB API 
   - Gá»­i dá»¯ liá»‡u phim vÃ o Kafka topic `movies`
   - Tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i genre IDs thÃ nh tÃªn

2. **Stream Processing**
   - Spark Streaming Ä‘á»c tá»« Kafka
   - Validate vÃ  transform dá»¯ liá»‡u
   - Ghi vÃ o MongoDB collection `movies.movie_data`

3. **Batch ETL**
   - Äá»c dá»¯ liá»‡u tá»« MongoDB
   - LÃ m sáº¡ch vÃ  chuáº©n hÃ³a
   - Explode genres (1 phim â†’ N records theo thá»ƒ loáº¡i)
   - LÆ°u vÃ o HDFS dáº¡ng JSONL

4. **Analytics**
   - MapReduce jobs phÃ¢n tÃ­ch trÃªn HDFS
   - 3 bÃ i toÃ¡n phÃ¢n tÃ­ch chÃ­nh

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

| CÃ´ng nghá»‡ | Version | Vai trÃ² |
|-----------|---------|---------|
| **Apache Kafka** | 7.3.2 | Message Queue |
| **Apache Spark** | 3.4.1 | Stream Processing & ETL |
| **Hadoop HDFS** | 3.2.1 | Distributed Storage |
| **Hadoop YARN** | 3.2.1 | Resource Manager |
| **MongoDB** | 6 | NoSQL Database |
| **Python** | 3.9 | Programming Language |
| **Docker** | Latest | Containerization |
| **Zookeeper** | 7.3.2 | Kafka Coordination |

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

- **Docker** & **Docker Compose** installed
- **Minimum**: 8GB RAM, 4 CPU cores
- **Recommended**: 16GB RAM, 8 CPU cores
- **Disk Space**: ~10GB free

## ğŸš€ CÃ i Ä‘áº·t vÃ  cháº¡y

### 1. Clone repository vÃ  chuáº©n bá»‹

```bash
git clone <repository-url>
cd <project-directory>
```

### 2. Cáº¥u hÃ¬nh TMDB API Key

Táº¡o file `.env` trong thÆ° má»¥c gá»‘c:

```bash
TMDB_API_KEY="your_api_key_here"
```

Hoáº·c sá»­ dá»¥ng API key máº·c Ä‘á»‹nh Ä‘Ã£ cÃ³ trong code (chá»‰ Ä‘á»ƒ test).

### 3. Khá»Ÿi Ä‘á»™ng toÃ n bá»™ há»‡ thá»‘ng

```bash
docker-compose up -d
```

### 4. Kiá»ƒm tra tráº¡ng thÃ¡i services

```bash
docker-compose ps
```

Chá» khoáº£ng 2-3 phÃºt Ä‘á»ƒ táº¥t cáº£ services khá»Ÿi Ä‘á»™ng hoÃ n táº¥t.

### 5. Theo dÃµi logs

```bash
# Producer logs
docker logs -f movie-producer

# Spark Streaming logs
docker logs -f spark-submit

# Kafka logs
docker logs -f kafka
```

### 6. Cháº¡y ETL tá»« MongoDB sang HDFS

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 \
  /opt/spark/work-dir/etl_mongodb_to_hdfs.py
```

### 7. Cháº¡y MapReduce jobs

**Job 1: Rating trung bÃ¬nh theo thá»ƒ loáº¡i**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q1 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q1_rating_by_genre.py \
    -mapper 'python3 mapreduce_q1_rating_by_genre.py' \
    -reducer 'python3 mapreduce_q1_rating_by_genre.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q1
  
  hdfs dfs -cat /output/q1/part-* > /mapreduce_scripts/results_q1.txt
"
```

**Job 2: Top 10 phim cÃ³ lÆ°á»£t vote cao nháº¥t theo nÄƒm**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q2 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q2_top10_votes_by_year.py \
    -mapper 'python3 mapreduce_q2_top10_votes_by_year.py' \
    -reducer 'python3 mapreduce_q2_top10_votes_by_year.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q2
  
  hdfs dfs -cat /output/q2/part-* > /mapreduce_scripts/results_q2.txt
"
```

**Job 3: PhÃ¢n phá»‘i phim theo khoáº£ng rating**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q3 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q3_rating_buckets.py \
    -mapper 'python3 mapreduce_q3_rating_buckets.py' \
    -reducer 'python3 mapreduce_q3_rating_buckets.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q3
  
  hdfs dfs -cat /output/q3/part-* > /mapreduce_scripts/results_q3.txt
"
```

### 8. Xem káº¿t quáº£

```bash
# Trong container
docker exec -it namenode cat /mapreduce_scripts/results_q1.txt
docker exec -it namenode cat /mapreduce_scripts/results_q2.txt
docker exec -it namenode cat /mapreduce_scripts/results_q3.txt

# Copy ra host
docker cp namenode:/mapreduce_scripts/results_q1.txt ./
docker cp namenode:/mapreduce_scripts/results_q2.txt ./
docker cp namenode:/mapreduce_scripts/results_q3.txt ./
```

## ğŸ“¦ CÃ¡c thÃ nh pháº§n chÃ­nh

### 1. Movie Producer (`producer/movie_producer.py`)

**Chá»©c nÄƒng:**
- Láº¥y dá»¯ liá»‡u phim tá»« TMDB API
- Chuyá»ƒn Ä‘á»•i genre IDs thÃ nh tÃªn
- Gá»­i message vÃ o Kafka topic `movies`
- TrÃ¡nh duplicate movies vá»›i set tracking

**Output format:**
```json
{
  "id": 12345,
  "title": "Movie Name",
  "year": 2024,
  "rating": 7.5,
  "votes": 1500,
  "genres": ["Action", "Adventure"],
  "popularity": 45.2,
  "source": "popular",
  "fetched_at": "2024-01-15 10:30:00"
}
```

### 2. Spark Streaming (`spark/spark_streaming_app.py`)

**Chá»©c nÄƒng:**
- Äá»c real-time stream tá»« Kafka
- Parse JSON vÃ  validate schema
- Transform dá»¯ liá»‡u
- Ghi vÃ o MongoDB vá»›i timestamp

**Xá»­ lÃ½:**
- Batch processing má»—i micro-batch
- Checkpoint Ä‘á»ƒ fault-tolerance
- Display progress logs

### 3. ETL Job (`spark/etl_mongodb_to_hdfs.py`)

**Pipeline:**
1. Äá»c tá»« MongoDB collection
2. Cast data types (IntegerType, DoubleType)
3. Validate data:
   - Year: 1900-2030
   - Rating: 0-10
   - Votes: >= 0
4. Explode genres (1 phim â†’ nhiá»u records)
5. Ghi vÃ o HDFS dáº¡ng JSONL

**Output schema:**
```
movie_id, title, original_title, year, rating, votes, 
genre, overview, popularity, original_language, 
release_date, source, fetched_at
```

### 4. MapReduce Scripts

**Location:** `mapreduce_scripts/`

**Äáº·c Ä‘iá»ƒm:**
- Sá»­ dá»¥ng Hadoop Streaming API
- Python implementation
- Class-based design (Mapper, Reducer)
- JSON parsing tá»« HDFS

## ğŸ“Š MapReduce Jobs

### Q1: Rating trung bÃ¬nh theo thá»ƒ loáº¡i

**Output format:**
```
Genre    AvgRating    Count
Action   7.25         1500
Drama    7.89         2300
```

**Logic:**
- Mapper: Emit (genre, rating, 1)
- Reducer: TÃ­nh avg, filter >= 50 movies

### Q2: Top 10 phim vote cao nháº¥t má»—i nÄƒm

**Output format:**
```
Year  Rank  Title          Votes    Rating
2024  1     Movie A        50000    8.5
2024  2     Movie B        45000    8.3
```

**Logic:**
- Mapper: Emit (year, votes|title|rating)
- Reducer: Sort by votes, láº¥y top 10

### Q3: PhÃ¢n phá»‘i rating

**Output format:**
```
Bucket   Count   Percentage
0-2      150     2.5%
2-4      500     8.3%
4-6      2000    33.3%
6-8      2500    41.7%
8-10     850     14.2%
```

**Logic:**
- Mapper: PhÃ¢n loáº¡i rating vÃ o bucket
- Reducer: Äáº¿m vÃ  tÃ­nh %

## ğŸ”‘ API Keys

Dá»± Ã¡n sá»­ dá»¥ng TMDB API. Äá»ƒ láº¥y API key:

1. ÄÄƒng kÃ½ táº¡i: https://www.themoviedb.org/signup
2. VÃ o Settings â†’ API
3. Request API key (miá»…n phÃ­)
4. ThÃªm vÃ o file `.env`

**Rate Limits:**
- 40 requests/10 seconds
- Producer Ä‘Ã£ config delay 5s giá»¯a cÃ¡c requests

## ğŸŒ Ports vÃ  Services

| Service | Port | URL | MÃ´ táº£ |
|---------|------|-----|-------|
| **Hadoop NameNode UI** | 9870 | http://localhost:9870 | HDFS Web UI |
| **YARN ResourceManager** | 8088 | http://localhost:8088 | YARN Jobs UI |
| **Spark Master UI** | 8080 | http://localhost:8080 | Spark Cluster UI |
| **Kafka** | 9092, 29092 | - | Kafka Broker |
| **MongoDB** | 27017 | - | Database |
| **Zookeeper** | 2181 | - | Coordination |

## ğŸ” Monitoring & Debugging

### 1. Kiá»ƒm tra Kafka topic

```bash
# List topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Consume messages
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic movies \
  --from-beginning \
  --max-messages 10
```

### 2. Kiá»ƒm tra HDFS

```bash
# List files
docker exec -it namenode hdfs dfs -ls /data/movies/silver/

# Read file content
docker exec -it namenode hdfs dfs -cat /data/movies/silver/movies.jsonl/part-*.json | head -20

# HDFS report
docker exec -it namenode hdfs dfsadmin -report
```

### 3. Kiá»ƒm tra MongoDB

```bash
# Access MongoDB shell
docker exec -it mongodb mongosh

# Inside mongosh:
use movies
db.movie_data.countDocuments()
db.movie_data.find().limit(5).pretty()
```

### 4. Check Spark Jobs

Truy cáº­p: http://localhost:8080
- Xem workers status
- Running applications
- Completed jobs

### 5. Check YARN Jobs

Truy cáº­p: http://localhost:8088
- Active applications
- Job history
- Resource usage

### Clear all data vÃ  restart

```bash
docker-compose down -v
rm -rf data/
docker-compose up -d
```

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```
.
â”œâ”€â”€ docker-compose.yml           # Orchestration
â”œâ”€â”€ .env                         # API keys
â”œâ”€â”€ hadoop.env                   # Hadoop config
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ movie_producer.py       # Kafka Producer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ spark_streaming_app.py  # Real-time processing
â”‚   â””â”€â”€ etl_mongodb_to_hdfs.py  # Batch ETL
â”œâ”€â”€ mapreduce_scripts/
â”‚   â”œâ”€â”€ mapreduce_q1_rating_by_genre.py
â”‚   â”œâ”€â”€ mapreduce_q2_top10_votes_by_year.py
â”‚   â””â”€â”€ mapreduce_q3_rating_buckets.py
â”œâ”€â”€ data/                        # HDFS data (gitignored)
â”‚   â”œâ”€â”€ namenode/
â”‚   â”œâ”€â”€ datanode/
â”‚   â””â”€â”€ historyserver/

```

## ğŸ“ Há»c gÃ¬ tá»« dá»± Ã¡n nÃ y?

1. **Lambda Architecture**: Káº¿t há»£p stream + batch processing
2. **Data Engineering**: ETL pipelines, data validation
3. **Distributed Systems**: Hadoop, Spark cluster
4. **Stream Processing**: Kafka + Spark Streaming
5. **NoSQL**: MongoDB document store
6. **Big Data**: MapReduce programming model
7. **DevOps**: Docker, containerization


**Created with â¤ï¸ for Data Engineer Learning**
