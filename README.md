# üé¨ Movie Data Pipeline - Real-time Streaming & Big Data Analytics

H·ªá th·ªëng x·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu phim theo th·ªùi gian th·ª±c, s·ª≠ d·ª•ng ki·∫øn tr√∫c Lambda v·ªõi Kafka, Spark Streaming, Hadoop MapReduce v√† MongoDB.

## üìã M·ª•c l·ª•c

- [T·ªïng quan](#-t·ªïng-quan)
- [Ki·∫øn tr√∫c h·ªá th·ªëng](#-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [C√¥ng ngh·ªá s·ª≠ d·ª•ng](#-c√¥ng-ngh·ªá-s·ª≠-d·ª•ng)
- [Y√™u c·∫ßu h·ªá th·ªëng](#-y√™u-c·∫ßu-h·ªá-th·ªëng)
- [C√†i ƒë·∫∑t v√† ch·∫°y](#-c√†i-ƒë·∫∑t-v√†-ch·∫°y)
- [C√°c th√†nh ph·∫ßn ch√≠nh](#-c√°c-th√†nh-ph·∫ßn-ch√≠nh)
- [MapReduce Jobs](#-mapreduce-jobs)
- [API Keys](#-api-keys)
- [Ports v√† Services](#-ports-v√†-services)


## üéØ T·ªïng quan

D·ª± √°n x√¢y d·ª±ng m·ªôt data pipeline ho√†n ch·ªânh ƒë·ªÉ:
- **Thu th·∫≠p** d·ªØ li·ªáu phim t·ª´ TMDB API theo th·ªùi gian th·ª±c
- **Streaming** d·ªØ li·ªáu qua Apache Kafka
- **X·ª≠ l√Ω real-time** v·ªõi Spark Structured Streaming
- **L∆∞u tr·ªØ** trong MongoDB (NoSQL)
- **ETL** t·ª´ MongoDB sang HDFS
- **Ph√¢n t√≠ch batch** v·ªõi Hadoop MapReduce

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

```
TMDB API
    ‚Üì
[Producer] ‚Üí Kafka ‚Üí [Spark Streaming] ‚Üí MongoDB
                                              ‚Üì
                                          [ETL Job]
                                              ‚Üì
                                            HDFS
                                              ‚Üì
                                      [MapReduce Jobs]
                                              ‚Üì
                                         Analytics
```

### Lu·ªìng d·ªØ li·ªáu chi ti·∫øt:

1. **Data Ingestion (Real-time)**
   - Producer g·ªçi TMDB API 
   - G·ª≠i d·ªØ li·ªáu phim v√†o Kafka topic `movies`
   - T·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi genre IDs th√†nh t√™n

2. **Stream Processing**
   - Spark Streaming ƒë·ªçc t·ª´ Kafka
   - Validate v√† transform d·ªØ li·ªáu
   - Ghi v√†o MongoDB collection `movies.movie_data`

3. **Batch ETL**
   - ƒê·ªçc d·ªØ li·ªáu t·ª´ MongoDB
   - L√†m s·∫°ch v√† chu·∫©n h√≥a
   - Explode genres (1 phim ‚Üí N records theo th·ªÉ lo·∫°i)
   - L∆∞u v√†o HDFS d·∫°ng JSONL

4. **Analytics**
   - MapReduce jobs ph√¢n t√≠ch tr√™n HDFS
   - 3 b√†i to√°n ph√¢n t√≠ch ch√≠nh

## üõ†Ô∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng

| C√¥ng ngh·ªá | Version | Vai tr√≤ |
|-----------|---------|---------|
| **Apache Kafka** | 7.3.2 | Message Queue |
| **Apache Spark** | 3.4.1 | Stream Processing & ETL |
| **Hadoop HDFS** | 3.2.1 | Distributed Storage |
| **Hadoop YARN** | 3.2.1 | Resource Manager |
| **MongoDB** | 6 | NoSQL Database |
| **Python** | 3.9 | Programming Language |
| **Docker** | Latest | Containerization |
| **Zookeeper** | 7.3.2 | Kafka Coordination |

## üíª Y√™u c·∫ßu h·ªá th·ªëng

- **Docker** & **Docker Compose** installed
- **Minimum**: 8GB RAM, 4 CPU cores
- **Recommended**: 16GB RAM, 8 CPU cores
- **Disk Space**: ~10GB free

## üöÄ C√†i ƒë·∫∑t v√† ch·∫°y

### 1. Clone repository v√† chu·∫©n b·ªã

```bash
git clone <repository-url>
cd <project-directory>
```

### 2. C·∫•u h√¨nh TMDB API Key

T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc:

```bash
TMDB_API_KEY="your_api_key_here"
```

Ho·∫∑c s·ª≠ d·ª•ng API key m·∫∑c ƒë·ªãnh ƒë√£ c√≥ trong code (ch·ªâ ƒë·ªÉ test).

### 3. Kh·ªüi ƒë·ªông to√†n b·ªô h·ªá th·ªëng

```bash
docker-compose up -d
```

### 4. Ki·ªÉm tra tr·∫°ng th√°i services

```bash
docker-compose ps
```

Ch·ªù kho·∫£ng 2-3 ph√∫t ƒë·ªÉ t·∫•t c·∫£ services kh·ªüi ƒë·ªông ho√†n t·∫•t.

### 5. Theo d√µi logs

```bash
# Producer logs
docker logs -f movie-producer

# Spark Streaming logs
docker logs -f spark-submit

# Kafka logs
docker logs -f kafka
```

### 6. Ch·∫°y ETL t·ª´ MongoDB sang HDFS

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 \
  /opt/spark/work-dir/etl_mongodb_to_hdfs.py
```

### 7. Ch·∫°y MapReduce jobs

**Job 1: Rating trung b√¨nh theo th·ªÉ lo·∫°i**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q1 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q1_rating_by_genre.py \
    -mapper 'python3 mapreduce_q1_rating_by_genre.py' \
    -reducer 'python3 mapreduce_q1_rating_by_genre.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q1
  
  hdfs dfs -cat /output/q1/part-* > /mapreduce_scripts/results_q1.txt
"
```

**Job 2: Top 10 phim c√≥ l∆∞·ª£t vote cao nh·∫•t theo nƒÉm**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q2 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q2_top10_votes_by_year.py \
    -mapper 'python3 mapreduce_q2_top10_votes_by_year.py' \
    -reducer 'python3 mapreduce_q2_top10_votes_by_year.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q2
  
  hdfs dfs -cat /output/q2/part-* > /mapreduce_scripts/results_q2.txt
"
```

**Job 3: Ph√¢n ph·ªëi phim theo kho·∫£ng rating**

```bash
docker exec -it namenode bash -c "
  hdfs dfs -rm -r /output/q3 2>/dev/null || true
  hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /mapreduce_scripts/mapreduce_q3_rating_buckets.py \
    -mapper 'python3 mapreduce_q3_rating_buckets.py' \
    -reducer 'python3 mapreduce_q3_rating_buckets.py reduce' \
    -input /data/movies/silver/movies.jsonl \
    -output /output/q3
  
  hdfs dfs -cat /output/q3/part-* > /mapreduce_scripts/results_q3.txt
"
```

### 8. Xem k·∫øt qu·∫£

```bash
# Trong container
docker exec -it namenode cat /mapreduce_scripts/results_q1.txt
docker exec -it namenode cat /mapreduce_scripts/results_q2.txt
docker exec -it namenode cat /mapreduce_scripts/results_q3.txt

# Copy ra host
docker cp namenode:/mapreduce_scripts/results_q1.txt ./
docker cp namenode:/mapreduce_scripts/results_q2.txt ./
docker cp namenode:/mapreduce_scripts/results_q3.txt ./
```

## üì¶ C√°c th√†nh ph·∫ßn ch√≠nh

### 1. Movie Producer (`producer/movie_producer.py`)

**Ch·ª©c nƒÉng:**
- L·∫•y d·ªØ li·ªáu phim t·ª´ TMDB API
- Chuy·ªÉn ƒë·ªïi genre IDs th√†nh t√™n
- G·ª≠i message v√†o Kafka topic `movies`
- Tr√°nh duplicate movies v·ªõi set tracking

**Output format:**
```json
{
  "id": 12345,
  "title": "Movie Name",
  "year": 2024,
  "rating": 7.5,
  "votes": 1500,
  "genres": ["Action", "Adventure"],
  "popularity": 45.2,
  "source": "popular",
  "fetched_at": "2024-01-15 10:30:00"
}
```

### 2. Spark Streaming (`spark/spark_streaming_app.py`)

**Ch·ª©c nƒÉng:**
- ƒê·ªçc real-time stream t·ª´ Kafka
- Parse JSON v√† validate schema
- Transform d·ªØ li·ªáu
- Ghi v√†o MongoDB v·ªõi timestamp

**X·ª≠ l√Ω:**
- Batch processing m·ªói micro-batch
- Checkpoint ƒë·ªÉ fault-tolerance
- Display progress logs

### 3. ETL Job (`spark/etl_mongodb_to_hdfs.py`)

**Pipeline:**
1. ƒê·ªçc t·ª´ MongoDB collection
2. Cast data types (IntegerType, DoubleType)
3. Validate data:
   - Year: 1900-2030
   - Rating: 0-10
   - Votes: >= 0
4. Explode genres (1 phim ‚Üí nhi·ªÅu records)
5. Ghi v√†o HDFS d·∫°ng JSONL

**Output schema:**
```
movie_id, title, original_title, year, rating, votes, 
genre, overview, popularity, original_language, 
release_date, source, fetched_at
```

### 4. MapReduce Scripts

**Location:** `mapreduce_scripts/`

**ƒê·∫∑c ƒëi·ªÉm:**
- S·ª≠ d·ª•ng Hadoop Streaming API
- Python implementation
- Class-based design (Mapper, Reducer)
- JSON parsing t·ª´ HDFS

## üìä MapReduce Jobs

### Q1: Rating trung b√¨nh theo th·ªÉ lo·∫°i

**Output format:**
```
Genre    AvgRating    Count
Action   7.25         1500
Drama    7.89         2300
```

**Logic:**
- Mapper: Emit (genre, rating, 1)
- Reducer: T√≠nh avg, filter >= 50 movies

### Q2: Top 10 phim vote cao nh·∫•t m·ªói nƒÉm

**Output format:**
```
Year  Rank  Title          Votes    Rating
2024  1     Movie A        50000    8.5
2024  2     Movie B        45000    8.3
```

**Logic:**
- Mapper: Emit (year, votes|title|rating)
- Reducer: Sort by votes, l·∫•y top 10

### Q3: Ph√¢n ph·ªëi rating

**Output format:**
```
Bucket   Count   Percentage
0-2      150     2.5%
2-4      500     8.3%
4-6      2000    33.3%
6-8      2500    41.7%
8-10     850     14.2%
```

**Logic:**
- Mapper: Ph√¢n lo·∫°i rating v√†o bucket
- Reducer: ƒê·∫øm v√† t√≠nh %

## üîë API Keys

D·ª± √°n s·ª≠ d·ª•ng TMDB API. ƒê·ªÉ l·∫•y API key:

1. ƒêƒÉng k√Ω t·∫°i: https://www.themoviedb.org/signup
2. V√†o Settings ‚Üí API
3. Request API key (mi·ªÖn ph√≠)
4. Th√™m v√†o file `.env`

**Rate Limits:**
- 40 requests/10 seconds
- Producer ƒë√£ config delay 5s gi·ªØa c√°c requests

## üåê Ports v√† Services

| Service | Port | URL | M√¥ t·∫£ |
|---------|------|-----|-------|
| **Hadoop NameNode UI** | 9870 | http://localhost:9870 | HDFS Web UI |
| **YARN ResourceManager** | 8088 | http://localhost:8088 | YARN Jobs UI |
| **Spark Master UI** | 8080 | http://localhost:8080 | Spark Cluster UI |
| **Kafka** | 9092, 29092 | - | Kafka Broker |
| **MongoDB** | 27017 | - | Database |
| **Zookeeper** | 2181 | - | Coordination |

## üîç Monitoring & Debugging

### 1. Ki·ªÉm tra Kafka topic

```bash
# List topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Consume messages
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic movies \
  --from-beginning \
  --max-messages 10
```

### 2. Ki·ªÉm tra HDFS

```bash
# List files
docker exec -it namenode hdfs dfs -ls /data/movies/silver/

# Read file content
docker exec -it namenode hdfs dfs -cat /data/movies/silver/movies.jsonl/part-*.json | head -20

# HDFS report
docker exec -it namenode hdfs dfsadmin -report
```

### 3. Ki·ªÉm tra MongoDB

```bash
# Access MongoDB shell
docker exec -it mongodb mongosh

# Inside mongosh:
use movies
db.movie_data.countDocuments()
db.movie_data.find().limit(5).pretty()
```

### 4. Check Spark Jobs

Truy c·∫≠p: http://localhost:8080
- Xem workers status
- Running applications
- Completed jobs

### 5. Check YARN Jobs

Truy c·∫≠p: http://localhost:8088
- Active applications
- Job history
- Resource usage

### Clear all data v√† restart

```bash
docker-compose down -v
rm -rf data/
docker-compose up -d
```

## üìÇ C·∫•u tr√∫c th∆∞ m·ª•c

```
.
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestration
‚îú‚îÄ‚îÄ .env                         # API keys
‚îú‚îÄ‚îÄ hadoop.env                   # Hadoop config
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ movie_producer.py       # Kafka Producer
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ spark_streaming_app.py  # Real-time processing
‚îÇ   ‚îî‚îÄ‚îÄ etl_mongodb_to_hdfs.py  # Batch ETL
‚îú‚îÄ‚îÄ mapreduce_scripts/
‚îÇ   ‚îú‚îÄ‚îÄ mapreduce_q1_rating_by_genre.py
‚îÇ   ‚îú‚îÄ‚îÄ mapreduce_q2_top10_votes_by_year.py
‚îÇ   ‚îî‚îÄ‚îÄ mapreduce_q3_rating_buckets.py
‚îú‚îÄ‚îÄ data/                        # HDFS data (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ namenode/
‚îÇ   ‚îú‚îÄ‚îÄ datanode/
‚îÇ   ‚îî‚îÄ‚îÄ historyserver/

```

## üéì H·ªçc g√¨ t·ª´ d·ª± √°n n√†y?

1. **Lambda Architecture**: K·∫øt h·ª£p stream + batch processing
2. **Data Engineering**: ETL pipelines, data validation
3. **Distributed Systems**: Hadoop, Spark cluster
4. **Stream Processing**: Kafka + Spark Streaming
5. **NoSQL**: MongoDB document store
6. **Big Data**: MapReduce programming model
7. **DevOps**: Docker, containerization


**Created with ‚ù§Ô∏è for Data Engineer Learning**
